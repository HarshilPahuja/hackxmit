# -*- coding: utf-8 -*-
"""deepfake detection

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ifrEs6bEWI0UoHaEVH-tuyIHm6hvmjHx
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.
import kagglehub
greatgamedota_faceforensics_path = kagglehub.dataset_download('greatgamedota/faceforensics')
nanduncs_1000_videos_split_path = kagglehub.dataset_download('nanduncs/1000-videos-split')
hungle3401_faceforensics_path = kagglehub.dataset_download('hungle3401/faceforensics')
sanikatiwarekar_deep_fake_detection_dfd_entire_original_dataset_path = kagglehub.dataset_download('sanikatiwarekar/deep-fake-detection-dfd-entire-original-dataset')
chandashekar8_deepfake_testing_videos_path = kagglehub.dataset_download('chandashekar8/deepfake-testing-videos')
anonnosingharay_deepfake_video_path = kagglehub.notebook_output_download('anonnosingharay/deepfake-video')

print('Data source import complete.')

"""## Data Preparation"""

import os
import cv2
import numpy as np
from tqdm import tqdm
from sklearn.model_selection import train_test_split, StratifiedShuffleSplit
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing.image import ImageDataGenerator


# Dataset paths
REAL_PATH = "/kaggle/input/deep-fake-detection-dfd-entire-original-dataset/DFD_original sequences"
FAKE_PATH = "/kaggle/input/deep-fake-detection-dfd-entire-original-dataset/DFD_manipulated_sequences/DFD_manipulated_sequences"
OUTPUT_FRAME_SIZE = (128, 128)
FRAME_COUNT = 10
MAX_VIDEOS = 10000

# ---------- 1. Optimized Frame Extraction ----------
def extract_frames_optimized(video_path, output_size=(128, 128), frame_count=10):
    """Extracts evenly spaced frames from a video"""
    cap = cv2.VideoCapture(video_path)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

    # Handle short videos
    if total_frames < frame_count:
        step = 1
    else:
        step = max(total_frames // frame_count, 1)

    frames = []
    positions = np.linspace(0, total_frames - 1, frame_count, dtype=int)

    for pos in positions:
        cap.set(cv2.CAP_PROP_POS_FRAMES, pos)
        ret, frame = cap.read()
        if not ret:
            break
        frame = cv2.resize(frame, output_size)
        frames.append(frame)

    cap.release()

    # Pad with black frames if insufficient frames
    while len(frames) < frame_count:
        frames.append(np.zeros((*output_size, 3), dtype=np.uint8))

    return np.array(frames)


# ---------- 2. Data Preparation ----------
def prepare_data(real_path, fake_path, max_videos=700):
    """Loads and processes real and fake videos"""
    data, labels = [], []

    # Real videos
    print("Processing real videos...")
    real_videos = os.listdir(real_path)[:max_videos]
    for video_file in tqdm(real_videos):
        video_path = os.path.join(real_path, video_file)
        frames = extract_frames_optimized(video_path, OUTPUT_FRAME_SIZE, FRAME_COUNT)
        if len(frames) == FRAME_COUNT:
            data.append(frames)
            labels.append(0)  # Real label

    # Fake videos
    print("Processing fake videos...")
    fake_videos = os.listdir(fake_path)[:max_videos]
    for video_file in tqdm(fake_videos):
        video_path = os.path.join(fake_path, video_file)
        frames = extract_frames_optimized(video_path, OUTPUT_FRAME_SIZE, FRAME_COUNT)
        if len(frames) == FRAME_COUNT:
            data.append(frames)
            labels.append(1)  # Fake label

    # Convert to numpy arrays
    data = np.array(data)
    labels = np.array(labels)
    return data, labels

# ---------- 4. Train-Test Split ----------
def split_data(X, y):
    """Splits data into training, validation, and test sets with stratified sampling"""
    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)

    y_flat = np.argmax(to_categorical(y, num_classes=2), axis=1)

    for train_idx, val_idx in sss.split(X.reshape(len(X), -1), y_flat):
        X_train, X_val = X[train_idx], X[val_idx]
        y_train, y_val = y[train_idx], y[val_idx]

    # Split validation into validation & test
    X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.5, random_state=42)

    return X_train, X_val, X_test, y_train, y_val, y_test


# ---------- 5. Data Standardization ----------
def standardize_data(X_train, X_val, X_test):
    """Standardizes pixel values"""
    scaler = StandardScaler()

    X_train_flat = X_train.reshape(-1, 128 * 128 * 3)
    X_val_flat = X_val.reshape(-1, 128 * 128 * 3)
    X_test_flat = X_test.reshape(-1, 128 * 128 * 3)

    X_train_scaled = scaler.fit_transform(X_train_flat).reshape(-1, FRAME_COUNT, 128, 128, 3)
    X_val_scaled = scaler.transform(X_val_flat).reshape(-1, FRAME_COUNT, 128, 128, 3)
    X_test_scaled = scaler.transform(X_test_flat).reshape(-1, FRAME_COUNT, 128, 128, 3)

    return X_train_scaled, X_val_scaled, X_test_scaled


# ---------- 6. Main Execution ----------
# Data preparation
X, y = prepare_data(REAL_PATH, FAKE_PATH)

# Splitting data
X_train, X_val, X_test, y_train, y_val, y_test = split_data(X, y)

# Standardize pixel values
X_train_scaled, X_val_scaled, X_test_scaled = standardize_data(X_train_aug, X_val, X_test)

# Convert labels to categorical
y_train = to_categorical(y_train_aug, num_classes=2)
y_val = to_categorical(y_val, num_classes=2)
y_test = to_categorical(y_test, num_classes=2)

# Save processed data
np.save('X_train.npy', X_train_scaled)
np.save('X_val.npy', X_val_scaled)
np.save('X_test.npy', X_test_scaled)
np.save('y_train.npy', y_train)
np.save('y_val.npy', y_val)
np.save('y_test.npy', y_test)

print(f"Final Shapes - Train: {X_train_scaled.shape}, Val: {X_val_scaled.shape}, Test: {X_test_scaled.shape}")

"""## Data Augmentation"""

import numpy as np
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Define a more diverse augmentation pipeline
datagen = ImageDataGenerator(
    horizontal_flip=True,
    vertical_flip=False,  # Avoid flipping if orientation matters
    rotation_range=15,  # Slightly more rotation
    zoom_range=0.2,  # More zooming
    brightness_range=[0.7, 1.3],  # Wider brightness range
    shear_range=10,  # Shearing helps with perspective variance
    width_shift_range=0.2,  # Allow more horizontal shifting
    height_shift_range=0.2,  # Allow more vertical shifting
    fill_mode="nearest"  # Avoid introducing black pixels
)

# Function to augment frames efficiently
def augment_frames(frames):
    return np.array([datagen.random_transform(frame) for frame in frames])

# Apply augmentation efficiently (avoid unnecessary memory duplication)
augmented_data = []
augmented_labels = []

for i in range(len(X_train)):
    for _ in range(2):  # Generate two augmentations per sample
        augmented_frames = augment_frames(X_train[i])  # Augment a sequence of frames
        augmented_data.append(augmented_frames)
        augmented_labels.append(y_train[i])  # Labels remain unchanged

# Convert to NumPy arrays
augmented_data = np.array(augmented_data)
augmented_labels = np.array(augmented_labels)

# Combine original and augmented data
X_train_augmented = np.concatenate((X_train, augmented_data), axis=0)
y_train_augmented = np.concatenate((y_train, augmented_labels), axis=0)

print(f"Original Train Data: {X_train.shape}")
print(f"Augmented Train Data: {X_train_augmented.shape}")  # Should be ~3x original size

"""## Model Architecture"""

import tensorflow as tf
from tensorflow.keras.applications import EfficientNetV2B0
from tensorflow.keras.layers import Dense, Flatten, TimeDistributed, LSTM, Dropout, BatchNormalization, Bidirectional
from tensorflow.keras.regularizers import l2
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers.schedules import CosineDecay

# Enable mixed precision for faster training on cloud GPUs
from tensorflow.keras import mixed_precision
mixed_precision.set_global_policy('mixed_float16')

# Parameters
FRAME_COUNT = 10  # Number of frames per video
INPUT_SHAPE = (FRAME_COUNT, 128, 128, 3)

# Define model
def build_optimized_model(input_shape=INPUT_SHAPE):
    # Use pre-trained ImageNet weights
    base_model = EfficientNetV2B0(weights='imagenet', include_top=False, input_shape=(128, 128, 3))

    model = Sequential([
        TimeDistributed(base_model),
        TimeDistributed(Flatten()),
        TimeDistributed(BatchNormalization()),

        Dropout(0.4),  # Regularization

        Bidirectional(LSTM(128, return_sequences=False,
                           recurrent_dropout=0.3,  # Regularization
                           kernel_regularizer=l2(0.001))),  # L2 regularization

        Dropout(0.4),

        Dense(128, activation='relu', kernel_regularizer=l2(0.001)),
        BatchNormalization(),
        Dropout(0.3),

        Dense(64, activation='relu', kernel_regularizer=l2(0.001)),
        Dropout(0.2),

        Dense(2, activation='softmax')
    ])

    # Optimizer with Cosine Decay
    initial_lr = 0.0001
    lr_schedule = CosineDecay(initial_learning_rate=initial_lr, decay_steps=5000, alpha=0.001)
    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule, clipnorm=1.0)

    model.compile(optimizer=optimizer,
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])

    return model

# Build and summarize the model
model = build_optimized_model()
model.summary()

"""## Model Training"""

from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau ,EarlyStopping

# Model checkpoint to save the best model in .keras format
checkpoint = ModelCheckpoint(
    "deepfake_detection_model.keras",  # Change to .keras
    monitor="val_accuracy",
    save_best_only=True,
    verbose=1
)

# Reduce learning rate on plateau
lr_scheduler = ReduceLROnPlateau(
    monitor="val_loss",
    factor=0.5,
    patience=3,
    verbose=1
)
early_stopping = EarlyStopping(
    monitor='val_accuracy',
    patience=5,   # Stops if no improvement for 5 epochs
    restore_best_weights=True
)
#For faster processing
from tensorflow.keras import mixed_precision
mixed_precision.set_global_policy('mixed_float16')


# Train the model
history = model.fit(
    X_train_augmented, y_train_augmented,
    validation_data=(X_val, y_val),
    epochs=40,
    batch_size=64,
    callbacks=[checkpoint, lr_scheduler , early_stopping]
)
model.save("deepfake_detection_model.keras")

"""## Model Testing"""

from sklearn.metrics import classification_report, accuracy_score

# Load the best saved model
from tensorflow.keras.models import load_model
model = load_model('deepfake_detection_model.keras')

# Evaluate on test set
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = np.argmax(y_test, axis=1)

# Metrics
accuracy = accuracy_score(y_true, y_pred_classes)
print(f"Test Accuracy: {accuracy * 100:.2f}%")

# Precision, Recall, F1-Score
print("Classification Report:")
print(classification_report(y_true, y_pred_classes, target_names=['REAL', 'FAKE']))

import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
import numpy as np

# Plot accuracy and loss graphs
def plot_training_history(history):
    # Accuracy
    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'], label='Train Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.title('Model Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    # Loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Model Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.tight_layout()
    plt.show()

# Compute confusion matrix
def plot_confusion_matrix(model, X_test, y_test):
    # Get predictions
    y_pred = model.predict(X_test)
    y_pred_classes = np.argmax(y_pred, axis=1)
    y_true = np.argmax(y_test, axis=1)

    # Compute confusion matrix
    cm = confusion_matrix(y_true, y_pred_classes)
    cm_labels = ['Real', 'Fake']

    # Plot confusion matrix
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=cm_labels, yticklabels=cm_labels)
    plt.title('Confusion Matrix')
    plt.xlabel('Predicted Labels')
    plt.ylabel('True Labels')
    plt.show()

    # Classification report
    print(classification_report(y_true, y_pred_classes, target_names=cm_labels))

# Plot training history
plot_training_history(history)

# Plot confusion matrix
plot_confusion_matrix(model, X_test, y_test)

"""## Real Time Detection

Training
"""

import os
import cv2
import numpy as np
from tqdm import tqdm
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Function to extract frames from a video
def extract_frames(video_path, output_size=(128, 128), frame_count=10):
    cap = cv2.VideoCapture(video_path)
    frames = []
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    step = max(total_frames // frame_count, 1)  # Uniform sampling

    for i in range(frame_count):
        cap.set(cv2.CAP_PROP_POS_FRAMES, i * step)
        ret, frame = cap.read()
        if not ret:
            break
        frame = cv2.resize(frame, output_size)
        frames.append(frame)
    cap.release()
    return np.array(frames)

from tensorflow.keras.models import load_model

# Load the model for real-time detection
loaded_model = load_model('deepfake_detection_model.keras')

def predict_video(video_path, model, output_size=(128, 128), frame_count=10):
    frames = extract_frames(video_path, output_size, frame_count)
    frames = frames / 255.0  # Normalize
    frames = np.expand_dims(frames, axis=0)  # Add batch dimension
    prediction = model.predict(frames)
    label = "FAKE" if np.argmax(prediction) == 1 else "REAL"
    confidence = prediction[0][np.argmax(prediction)]
    print(f"Prediction: {label} (Confidence: {confidence:.2f})")

REAL_PATH = "/kaggle/input/faceforensics/FF++/real"
FAKE_PATH = "/kaggle/input/faceforensics/FF++/fake"
# Test prediction on a video
real_sample_path = os.path.join(REAL_PATH, "/kaggle/input/deepfake-testing-videos/model1.mp4")  # Replace with real video path
fake_sample_path = os.path.join(FAKE_PATH, "/kaggle/input/faceforensics/FF++/fake
print("Real Video Prediction:")
predict_video(real_sample_path,loaded_model)

print("Fake Video Prediction:")
predict_video(fake_sample_path,loaded_model)

"""Training Data Link

Real Videos - "/kaggle/input/deep-fake-detection-dfd-entire-original-dataset/DFD_original sequences"

Fake Videos - /kaggle/input/deep-fake-detection-dfd-entire-original-dataset/DFD_manipulated_sequences/DFD_manipulated_sequences

 Real Time Testing Data

 Real Video - /kaggle/input/deepfake-testing-videos/model1.mp4"\
 Fake Video - /kaggle/input/faceforensics/FF++/fake

 Reference -


```
# This is formatted as code
```


"""

